{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c7b94b-a434-4a23-8956-5f237454163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python Libraries\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Processing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, Sampler\n",
    "\n",
    "# Progress Tracker\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ee387c-c584-4645-88ab-51d90f3c26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data_dict contains data from 1 COMID(s)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Define file_path\n",
    "input_file_path = \"/N/lustre/project/proj-212/praval/Global_sediment_flux/lstm_model/codes/cluster/Hydrogeosciences_codes/input_data_sample_COMID_74068275.0.csv\"\n",
    "data_dir = os.path.dirname(input_file_path)\n",
    "\n",
    "# Function to read and process a single file\n",
    "def read_file(file_name, data_dir):\n",
    "    new_data_file = os.path.join(data_dir, file_name)\n",
    "    df = pd.read_csv(new_data_file)\n",
    "\n",
    "    COMID = int(df.iloc[0, 0])  #COMID is the first column\n",
    "\n",
    "    # Process dates\n",
    "    dates = [datetime.date(*map(int, date_str.split('-'))).toordinal() for date_str in df.iloc[:, 1]]\n",
    "\n",
    "    # Extract data (All columns after the second are numerical data)\n",
    "    data = df.iloc[:, 2:].values\n",
    "\n",
    "    # Select COMID with at least a certain number of observations\n",
    "    valid_observations = np.count_nonzero(\n",
    "        [str(val).strip().lower() != 'nan' for val in data[:, 0]]\n",
    "    )\n",
    "    \n",
    "    if valid_observations < 50:\n",
    "        return None  # Skip files with insufficient valid observations\n",
    "\n",
    "    return COMID, np.array(dates), torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "file_name = os.path.basename(input_file_path)\n",
    "\n",
    "# Dictionary to store data\n",
    "data_dict = {}\n",
    "date_indices = None\n",
    "\n",
    "# Process the file\n",
    "result = read_file(file_name, data_dir)\n",
    "\n",
    "if result is not None:\n",
    "    COMID, dates, data = result\n",
    "    data_dict[COMID] = {\n",
    "        'dates': dates,\n",
    "        'data': data\n",
    "    }\n",
    "\n",
    "    if date_indices is None:\n",
    "        date_indices = dates\n",
    "\n",
    "# Sort the dictionary by COMID\n",
    "data_dict = OrderedDict(sorted(data_dict.items()))\n",
    "\n",
    "# Output results\n",
    "print(f\"Final data_dict contains data from {len(data_dict)} COMID(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3591e8-401a-4a56-a9a8-49989316cacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing COMIDs: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 COMIDs into train and test days.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the sequence length\n",
    "Lseq = 30  # Sequence length\n",
    "\n",
    "# Define the training fraction\n",
    "x = 0.7  # Percentage of data to be used for training\n",
    "\n",
    "# Dictionaries to store training and testing days for each COMID\n",
    "train_days = {}\n",
    "test_days = {}\n",
    "\n",
    "# Function to process a single COMID\n",
    "def process_comid(comid, comid_data):\n",
    "    dates = comid_data['dates']  # Corresponding ordinal dates\n",
    "    flux = comid_data['data'][:, 0]  # Observed_Sediment_Flux is the first column of 'data'\n",
    "\n",
    "    # Step 1: Find the rows where Observed_Sediment_Flux is not 'nan' and get the corresponding dates\n",
    "    valid_days = [day for day in range(len(flux)) if not torch.isnan(flux[day])]\n",
    "    valid_dates = [dates[day] for day in valid_days]\n",
    "\n",
    "    # Step 2: Compute the training and testing split\n",
    "    total_valid_count = len(valid_dates)\n",
    "    target_training_count = int(x * total_valid_count)\n",
    "\n",
    "    # Step 3: Split valid dates into training and testing sets\n",
    "    training_dates = valid_dates[:target_training_count]\n",
    "    testing_dates = valid_dates[target_training_count:]\n",
    "\n",
    "    # Filter dates that correspond to indices greater than or equal to Lseq - 1\n",
    "    training_dates_filtered = [date for idx, date in enumerate(training_dates) if valid_days[idx] >= (Lseq - 1)]\n",
    "    testing_dates_filtered = [date for idx, date in enumerate(testing_dates) if valid_days[idx] >= (Lseq - 1)]\n",
    "\n",
    "    return comid, training_dates_filtered, testing_dates_filtered\n",
    "\n",
    "# Process COMIDs in parallel using ProcessPoolExecutor\n",
    "with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    # Submit tasks for each COMID\n",
    "    futures = {executor.submit(process_comid, comid, comid_data): comid for comid, comid_data in data_dict.items()}\n",
    "\n",
    "    # Collect results as they complete\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing COMIDs\"):\n",
    "        comid, training_dates_filtered, testing_dates_filtered = future.result()\n",
    "        train_days[comid] = training_dates_filtered\n",
    "        test_days[comid] = testing_dates_filtered\n",
    "\n",
    "# Final Summary\n",
    "print(f\"Processed {len(train_days)} COMIDs into train and test days.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b521aa7-fcc1-4508-afab-cae163c7cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing COMIDs: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean (Inputs): [ 1.35882950e+02  1.46200012e+03  4.26106334e-01  3.45718674e-03\n",
      "  3.04222514e-04  2.12073612e+00  1.15095437e+00  0.00000000e+00\n",
      "  3.66467573e-02  9.85087872e+00  8.17691803e+00  6.57772903e+01\n",
      "  4.02774841e-01  1.06712523e+01  4.20336097e-01  4.09233153e-01\n",
      "  0.00000000e+00  6.67649433e-02  4.86338496e-01  1.79957043e+03\n",
      "  2.90231567e+02  2.89522888e+02  2.89654968e+02  6.30249023e+02\n",
      "  1.02759106e-04  5.34736589e-02  2.85216477e-02  0.00000000e+00\n",
      "  1.50612640e+01  1.77915648e-01  2.05981421e+00  7.95171127e+01\n",
      "  8.09491813e-01  2.29233503e+00  0.00000000e+00  7.06856226e-05\n",
      "  0.00000000e+00  1.42251550e+06  1.47047016e+05  7.36038093e+10\n",
      "  2.71730846e-06 -1.36232071e+01  4.88356600e+07  2.96302980e+07\n",
      "  1.51104193e+01  3.99572074e-01  2.20545721e+00  1.21800000e+03\n",
      "  8.19000000e+02  1.40100000e+03  1.39800000e+03  1.00800000e+03\n",
      "  6.88000000e+02  4.03000000e+02  8.70000000e+01  5.90000000e+01\n",
      "  6.80000000e+01  4.68000000e+02  1.07000000e+02  1.00000000e+01\n",
      "  1.30000000e+01  2.00000000e+00  4.00000000e+00  1.90000000e+01\n",
      "  4.90000000e+01  3.30000000e+01  9.00000000e+00  4.10000000e+01\n",
      "  2.50000000e+01  2.20000000e+01  4.30000000e+01  3.70000000e+01\n",
      "  3.20000000e+01  4.00000000e+01  2.92880000e+04  1.25400000e+03\n",
      "  1.00000000e+00  8.75200000e+03  1.57200000e+03  1.60100007e+00\n",
      "  1.04349995e+01  1.00000000e+00 -9.26452412e-05  3.32109161e+02\n",
      "  2.01502243e+02]\n",
      "Overall Standard Deviation (Inputs): [1.0000000e-03 1.0000000e-03 9.6373416e-02 1.2214321e-03 1.0000000e-03\n",
      " 7.4159145e-01 2.6015380e-01 1.0000000e-03 1.7760005e-02 1.2582200e+00\n",
      " 3.7078348e-01 1.1934295e+00 6.4855181e-02 5.6008255e-01 1.0000000e-03\n",
      " 2.0716505e-01 1.0000000e-03 5.5692331e-03 1.5274341e-02 1.0000000e-03\n",
      " 9.1552200e+00 9.1327019e+00 9.2343740e+00 6.2552521e+02 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 5.0437320e+04 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 7.4784036e+00 8.1207310e+06 1.1419814e+07 2.9536959e+01\n",
      " 3.0743859e+00 4.6785436e+00 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03 1.0000000e-03\n",
      " 1.0000000e-03 1.0000000e-03 1.0000000e-03 5.5225407e+01 7.7660973e+01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List to collect all training data across COMIDs\n",
    "all_training_data = []\n",
    "\n",
    "# Dictionary to store max valid_y_values for each COMID\n",
    "comid_max_y_values = {}\n",
    "\n",
    "# Iterate over each COMID in the data_dict\n",
    "for comid, comid_data in tqdm(data_dict.items(), desc=\"Processing COMIDs\"):\n",
    "    # Get the last training day index from train_days\n",
    "    last_training_date = train_days[comid][-1]  # Last training date\n",
    "    last_training_index = np.where(comid_data['dates'] == last_training_date)[0][0]  # Index of the last training day\n",
    "\n",
    "    # Extract the data up to the last training row\n",
    "    training_data = comid_data['data'][:last_training_index + 1]  # All rows up to last_training_index\n",
    "\n",
    "    # Separate inputs (excluding the first column)\n",
    "    input_data = training_data[:, 1:]  # All columns except the first\n",
    "\n",
    "    # Extract the first column (target values)\n",
    "    y_values = training_data[:, 0]  # First column is the target\n",
    "\n",
    "    # Filter out rows with 'nan' values for the target column\n",
    "    valid_y_values = [y.item() for y in y_values if str(y.item()).strip().lower() != 'nan']\n",
    "\n",
    "    # Compute and store the max of valid_y_values for the current COMID\n",
    "    if valid_y_values:  # Ensure there are valid values\n",
    "        comid_max_y_values[comid] = max(valid_y_values)  \n",
    "\n",
    "    # Collect input data\n",
    "    all_training_data.append(input_data)\n",
    "\n",
    "# Combine all training data into a single array\n",
    "all_training_data = np.vstack(all_training_data)\n",
    "\n",
    "# Compute the overall mean and standard deviation for inputs\n",
    "overall_mean = np.nanmean(all_training_data, axis=0)\n",
    "overall_std = np.nanstd(all_training_data, axis=0)\n",
    "\n",
    "# Apply the threshold to the standard deviation\n",
    "overall_std[overall_std < 0.001] = 0.001\n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Mean (Inputs): {overall_mean}\")\n",
    "print(f\"Overall Standard Deviation (Inputs): {overall_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c47918a-1f0b-4737-ade5-d43b115663c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Dataset):\n",
    "    def __init__(self, data_dict, days, seq_len, mean_inputs, std_inputs, comid_max_y_values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dict: Dictionary containing 'data' and 'dates' for each COMID.\n",
    "            days: Dictionary containing valid indices for each COMID.\n",
    "            seq_len: Length of the input sequence.\n",
    "            mean_inputs: Mean values for normalization of inputs.\n",
    "            std_inputs: Standard deviation values for normalization of inputs.\n",
    "            comid_max_y_values: Maximum target value for current COMID    \n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_dict = data_dict\n",
    "        self.days = days\n",
    "        self.seq_len = seq_len\n",
    "        self.mean_inputs = mean_inputs\n",
    "        self.std_inputs = std_inputs\n",
    "        self.comid_max_y_values = comid_max_y_values\n",
    "        self.indices = [\n",
    "            (comid, torch.where(torch.tensor(self.data_dict[comid]['dates']) == day)[0].item())\n",
    "            for comid, day_list in days.items() for day in day_list\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            comid: The COMID of the sample.\n",
    "            x: Normalized input sequence of shape (seq_len, num_features).\n",
    "            y: Target value corresponding to the end of the sequence, normalized.\n",
    "            y_date: The ordinal date of the target value y.       \n",
    "        \"\"\"\n",
    "        comid, day_index = self.indices[idx]\n",
    "        data = self.data_dict[comid]['data']\n",
    "        dates = self.data_dict[comid]['dates']\n",
    "\n",
    "        # Extract input sequence and target\n",
    "        x = data[day_index - self.seq_len + 1:day_index + 1, 1:]  # Inputs\n",
    "        y = data[day_index, 0]  # Target\n",
    "        y_date = dates[day_index]  # Date of the target value\n",
    "\n",
    "        # Normalize inputs\n",
    "        x_normalized = (x - self.mean_inputs) / self.std_inputs\n",
    "\n",
    "        # Define a small epsilon to handle zero target values\n",
    "        epsilon = 1e-3\n",
    "\n",
    "        # Check if y is 0 and add epsilon if so\n",
    "        if y == 0:\n",
    "            y = epsilon  # Avoid zero target values\n",
    "\n",
    "        y = y.clone().detach().float() if isinstance(y, torch.Tensor) else torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Get the max y value for the current COMID\n",
    "        max_y = self.comid_max_y_values.get(comid, 1.0)  # Default to 1.0 if not found\n",
    "\n",
    "        # Normalize target by the COMID-specific max value\n",
    "        y_normalized = y / max_y if max_y > 0 else y  # Avoid division by zero    \n",
    "\n",
    "        # Return comid as a tensor as well\n",
    "        comid_tensor = torch.tensor(comid, dtype=torch.long)\n",
    "\n",
    "        # Return comid, normalized inputs, normalized target\n",
    "        return comid_tensor, x_normalized, y_normalized, y_date\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = CustomData(\n",
    "    data_dict=data_dict,\n",
    "    days=train_days,\n",
    "    seq_len=Lseq,\n",
    "    mean_inputs=overall_mean,\n",
    "    std_inputs=overall_std,\n",
    "    comid_max_y_values=comid_max_y_values\n",
    ")\n",
    "\n",
    "test_dataset = CustomData(\n",
    "    data_dict=data_dict,\n",
    "    days=test_days,\n",
    "    seq_len=Lseq,\n",
    "    mean_inputs=overall_mean,\n",
    "    std_inputs=overall_std,\n",
    "    comid_max_y_values=comid_max_y_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d27f2b-76bf-4e97-a72c-06dc9dac7934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of indices in train_dataset: 137\n",
      "Number of indices in test_dataset: 60\n"
     ]
    }
   ],
   "source": [
    "# Print the length of indices for both train and test datasets\n",
    "train_indices_length = len(train_dataset.indices)\n",
    "test_indices_length = len(test_dataset.indices)\n",
    "\n",
    "# Output the lengths\n",
    "print(f\"Number of indices in train_dataset: {train_indices_length}\")\n",
    "print(f\"Number of indices in test_dataset: {test_indices_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b011ef3-2b7f-48f5-b0dc-4b92075b0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = 0.40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim, device = x.device).requires_grad_()\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim, device = x.device).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(self.dropout(hn[0,:,:]))\n",
    "        return out\n",
    "\n",
    "# Custom NSE computation function\n",
    "def computeNSE(obs, pred):\n",
    "    sse = np.sum((obs - pred)**2)\n",
    "    sst = np.sum((obs - np.mean(obs))**2)\n",
    "    nse = 1 - sse / sst\n",
    "    return nse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fde54a6-2793-413a-96af-7a03c292f958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|█▉                                                              | 3/100 [00:00<00:10,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Avg Loss: 0.2404, NSE: -1.2941, RMSE: 2.3086\n",
      "Epoch 2/100, Avg Loss: 0.1973, NSE: -0.8437, RMSE: 2.0923\n",
      "Epoch 3/100, Avg Loss: 0.2043, NSE: -0.6474, RMSE: 2.2857\n",
      "Epoch 4/100, Avg Loss: 0.2028, NSE: -0.5875, RMSE: 2.8359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|████▍                                                           | 7/100 [00:00<00:06, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Avg Loss: 0.1836, NSE: -0.5117, RMSE: 3.2852\n",
      "Epoch 6/100, Avg Loss: 0.1851, NSE: -0.3992, RMSE: 2.5230\n",
      "Epoch 7/100, Avg Loss: 0.1696, NSE: -0.4180, RMSE: 2.5942\n",
      "Epoch 8/100, Avg Loss: 0.1818, NSE: -0.3104, RMSE: 1.7452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|██████▉                                                        | 11/100 [00:00<00:05, 14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Avg Loss: 0.1734, NSE: -0.3100, RMSE: 1.8145\n",
      "Epoch 10/100, Avg Loss: 0.1691, NSE: -0.1797, RMSE: 2.2399\n",
      "Epoch 11/100, Avg Loss: 0.1572, NSE: -0.1844, RMSE: 2.5713\n",
      "Epoch 12/100, Avg Loss: 0.1471, NSE: -0.0693, RMSE: 3.1166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█████████▍                                                     | 15/100 [00:01<00:05, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Avg Loss: 0.1449, NSE: -0.0470, RMSE: 2.9024\n",
      "Epoch 14/100, Avg Loss: 0.1395, NSE: -0.0182, RMSE: 2.7000\n",
      "Epoch 15/100, Avg Loss: 0.1429, NSE: 0.0175, RMSE: 2.1058\n",
      "Epoch 16/100, Avg Loss: 0.1550, NSE: 0.0373, RMSE: 2.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|███████████▉                                                   | 19/100 [00:01<00:05, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Avg Loss: 0.1237, NSE: 0.1055, RMSE: 2.2321\n",
      "Epoch 18/100, Avg Loss: 0.1284, NSE: 0.1082, RMSE: 2.2127\n",
      "Epoch 19/100, Avg Loss: 0.1173, NSE: 0.1605, RMSE: 1.7315\n",
      "Epoch 20/100, Avg Loss: 0.1194, NSE: 0.2057, RMSE: 2.2954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|██████████████▍                                                | 23/100 [00:01<00:04, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Avg Loss: 0.1146, NSE: 0.3026, RMSE: 1.8344\n",
      "Epoch 22/100, Avg Loss: 0.1217, NSE: 0.2738, RMSE: 1.7749\n",
      "Epoch 23/100, Avg Loss: 0.1157, NSE: 0.2688, RMSE: 2.5580\n",
      "Epoch 24/100, Avg Loss: 0.1184, NSE: 0.3217, RMSE: 2.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|█████████████████                                              | 27/100 [00:01<00:04, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Avg Loss: 0.0992, NSE: 0.3944, RMSE: 1.7672\n",
      "Epoch 26/100, Avg Loss: 0.1046, NSE: 0.4097, RMSE: 2.5789\n",
      "Epoch 27/100, Avg Loss: 0.1111, NSE: 0.3597, RMSE: 2.2454\n",
      "Epoch 28/100, Avg Loss: 0.1078, NSE: 0.3311, RMSE: 1.9855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  31%|███████████████████▌                                           | 31/100 [00:02<00:04, 16.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Avg Loss: 0.1185, NSE: 0.3822, RMSE: 1.8795\n",
      "Epoch 30/100, Avg Loss: 0.1137, NSE: 0.4030, RMSE: 1.9341\n",
      "Epoch 31/100, Avg Loss: 0.1146, NSE: 0.4302, RMSE: 2.0371\n",
      "Epoch 32/100, Avg Loss: 0.1040, NSE: 0.3641, RMSE: 2.0254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|██████████████████████                                         | 35/100 [00:02<00:03, 16.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Avg Loss: 0.1087, NSE: 0.3488, RMSE: 1.6566\n",
      "Epoch 34/100, Avg Loss: 0.1249, NSE: 0.3486, RMSE: 1.5270\n",
      "Epoch 35/100, Avg Loss: 0.1117, NSE: 0.4123, RMSE: 1.9312\n",
      "Epoch 36/100, Avg Loss: 0.1023, NSE: 0.4249, RMSE: 2.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|████████████████████████▌                                      | 39/100 [00:02<00:03, 16.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Avg Loss: 0.1051, NSE: 0.4393, RMSE: 1.9942\n",
      "Epoch 38/100, Avg Loss: 0.1151, NSE: 0.4115, RMSE: 1.6928\n",
      "Epoch 39/100, Avg Loss: 0.1056, NSE: 0.4486, RMSE: 1.4339\n",
      "Epoch 40/100, Avg Loss: 0.1099, NSE: 0.3745, RMSE: 1.7570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|███████████████████████████                                    | 43/100 [00:02<00:03, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Avg Loss: 0.1017, NSE: 0.4188, RMSE: 1.4453\n",
      "Epoch 42/100, Avg Loss: 0.0890, NSE: 0.4546, RMSE: 1.4185\n",
      "Epoch 43/100, Avg Loss: 0.1161, NSE: 0.4314, RMSE: 2.2941\n",
      "Epoch 44/100, Avg Loss: 0.1094, NSE: 0.4449, RMSE: 1.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|█████████████████████████████▌                                 | 47/100 [00:03<00:03, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Avg Loss: 0.1030, NSE: 0.4453, RMSE: 1.4731\n",
      "Epoch 46/100, Avg Loss: 0.0953, NSE: 0.4314, RMSE: 1.4002\n",
      "Epoch 47/100, Avg Loss: 0.0956, NSE: 0.4567, RMSE: 1.4774\n",
      "Epoch 48/100, Avg Loss: 0.0981, NSE: 0.4118, RMSE: 1.3895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  51%|████████████████████████████████▏                              | 51/100 [00:03<00:02, 16.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Avg Loss: 0.0979, NSE: 0.4314, RMSE: 1.4510\n",
      "Epoch 50/100, Avg Loss: 0.0988, NSE: 0.4544, RMSE: 1.9050\n",
      "Epoch 51/100, Avg Loss: 0.1041, NSE: 0.4047, RMSE: 1.6348\n",
      "Epoch 52/100, Avg Loss: 0.0924, NSE: 0.4411, RMSE: 1.5442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|██████████████████████████████████▋                            | 55/100 [00:03<00:02, 17.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Avg Loss: 0.0965, NSE: 0.4235, RMSE: 1.2750\n",
      "Epoch 54/100, Avg Loss: 0.1011, NSE: 0.4644, RMSE: 1.5605\n",
      "Epoch 55/100, Avg Loss: 0.0958, NSE: 0.4330, RMSE: 1.5525\n",
      "Epoch 56/100, Avg Loss: 0.1039, NSE: 0.4548, RMSE: 1.6139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  59%|█████████████████████████████████████▏                         | 59/100 [00:03<00:02, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Avg Loss: 0.0971, NSE: 0.4696, RMSE: 1.6193\n",
      "Epoch 58/100, Avg Loss: 0.0861, NSE: 0.4784, RMSE: 1.4043\n",
      "Epoch 59/100, Avg Loss: 0.0935, NSE: 0.4088, RMSE: 1.5623\n",
      "Epoch 60/100, Avg Loss: 0.0931, NSE: 0.4281, RMSE: 1.4222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  63%|███████████████████████████████████████▋                       | 63/100 [00:03<00:02, 17.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Avg Loss: 0.1047, NSE: 0.4309, RMSE: 1.1884\n",
      "Epoch 62/100, Avg Loss: 0.1041, NSE: 0.4379, RMSE: 1.3229\n",
      "Epoch 63/100, Avg Loss: 0.1053, NSE: 0.4162, RMSE: 1.1760\n",
      "Epoch 64/100, Avg Loss: 0.0875, NSE: 0.4525, RMSE: 1.7178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████████████████████████████████████████▏                    | 67/100 [00:04<00:01, 18.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Avg Loss: 0.0983, NSE: 0.4628, RMSE: 1.5840\n",
      "Epoch 66/100, Avg Loss: 0.0958, NSE: 0.4649, RMSE: 1.5997\n",
      "Epoch 67/100, Avg Loss: 0.1136, NSE: 0.4776, RMSE: 1.4486\n",
      "Epoch 68/100, Avg Loss: 0.0928, NSE: 0.4370, RMSE: 1.4077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  71%|████████████████████████████████████████████▋                  | 71/100 [00:04<00:01, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Avg Loss: 0.0963, NSE: 0.4799, RMSE: 1.1769\n",
      "Epoch 70/100, Avg Loss: 0.1053, NSE: 0.4644, RMSE: 1.2442\n",
      "Epoch 71/100, Avg Loss: 0.1103, NSE: 0.4057, RMSE: 1.3010\n",
      "Epoch 72/100, Avg Loss: 0.1002, NSE: 0.4874, RMSE: 1.6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████████████████████████████████████████████▎               | 75/100 [00:04<00:01, 18.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Avg Loss: 0.1003, NSE: 0.5036, RMSE: 1.5860\n",
      "Epoch 74/100, Avg Loss: 0.0940, NSE: 0.4584, RMSE: 1.8198\n",
      "Epoch 75/100, Avg Loss: 0.1002, NSE: 0.4412, RMSE: 1.4600\n",
      "Epoch 76/100, Avg Loss: 0.1019, NSE: 0.4620, RMSE: 1.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  79%|█████████████████████████████████████████████████▊             | 79/100 [00:04<00:01, 18.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Avg Loss: 0.0908, NSE: 0.5107, RMSE: 1.3453\n",
      "Epoch 78/100, Avg Loss: 0.0952, NSE: 0.4750, RMSE: 1.4194\n",
      "Epoch 79/100, Avg Loss: 0.0967, NSE: 0.4601, RMSE: 1.4155\n",
      "Epoch 80/100, Avg Loss: 0.0852, NSE: 0.5151, RMSE: 1.2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  83%|████████████████████████████████████████████████████▎          | 83/100 [00:05<00:00, 19.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Avg Loss: 0.0944, NSE: 0.5001, RMSE: 1.0936\n",
      "Epoch 82/100, Avg Loss: 0.0841, NSE: 0.4816, RMSE: 1.2609\n",
      "Epoch 83/100, Avg Loss: 0.0868, NSE: 0.4780, RMSE: 1.3934\n",
      "Epoch 84/100, Avg Loss: 0.0947, NSE: 0.4793, RMSE: 1.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  87%|██████████████████████████████████████████████████████▊        | 87/100 [00:05<00:00, 19.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Avg Loss: 0.1022, NSE: 0.4673, RMSE: 1.2061\n",
      "Epoch 86/100, Avg Loss: 0.0943, NSE: 0.4322, RMSE: 1.3632\n",
      "Epoch 87/100, Avg Loss: 0.1000, NSE: 0.4303, RMSE: 0.9456\n",
      "Epoch 88/100, Avg Loss: 0.1034, NSE: 0.4399, RMSE: 1.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████████████████████████████████████████████████████▎     | 91/100 [00:05<00:00, 19.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Avg Loss: 0.1084, NSE: 0.4416, RMSE: 1.3837\n",
      "Epoch 90/100, Avg Loss: 0.1045, NSE: 0.4990, RMSE: 1.4233\n",
      "Epoch 91/100, Avg Loss: 0.0889, NSE: 0.4855, RMSE: 1.2483\n",
      "Epoch 92/100, Avg Loss: 0.0930, NSE: 0.4676, RMSE: 1.4373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|███████████████████████████████████████████████████████████▊   | 95/100 [00:05<00:00, 19.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Avg Loss: 0.1062, NSE: 0.4819, RMSE: 1.7435\n",
      "Epoch 94/100, Avg Loss: 0.0795, NSE: 0.5262, RMSE: 1.3355\n",
      "Epoch 95/100, Avg Loss: 0.0962, NSE: 0.5021, RMSE: 1.2952\n",
      "Epoch 96/100, Avg Loss: 0.0900, NSE: 0.4836, RMSE: 1.3632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 16.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Avg Loss: 0.0986, NSE: 0.4688, RMSE: 1.3319\n",
      "Epoch 98/100, Avg Loss: 0.0936, NSE: 0.4770, RMSE: 1.3020\n",
      "Epoch 99/100, Avg Loss: 0.0895, NSE: 0.5529, RMSE: 1.2875\n",
      "Epoch 100/100, Avg Loss: 0.0806, NSE: 0.5410, RMSE: 1.2376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # Ensures consistency across GPUs\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Set device before using it in the dataset class\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = len(overall_mean) # Number of features for the LSTM input\n",
    "hidden_dim = 256  # Hidden dimension for LSTM\n",
    "n_layers = 1  # Number of layers for LSTM\n",
    "output_dim = 1  # Single target value output\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "N = 2**5  # Adjust batch size as needed\n",
    "\n",
    "# Initialize LSTM model\n",
    "lstm_model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, n_layers=n_layers, output_dim=output_dim)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=N, sampler=train_sampler)\n",
    "\n",
    "# Training loop\n",
    "model_state = []\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "    lstm_model.train()  # Set model to training mode\n",
    "    total_loss = 0.0  # Accumulate loss for the entire epoch\n",
    "    all_predictions = []  # Store all predictions for the epoch\n",
    "    all_observations = []  # Store all true values for the epoch    \n",
    "\n",
    "    for batch_idx, (comid, x_batch, y_batch, y_date) in enumerate(train_dataloader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device).view(-1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = lstm_model(x_batch)  # Get predictions\n",
    "\n",
    "        #print (predictions)\n",
    "        loss = criterion(predictions, y_batch) # Compute loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and true values for analysis\n",
    "        all_predictions.append(predictions.detach().cpu())  # Move to CPU\n",
    "        all_observations.append(y_batch.detach().cpu())  # Move to CPU\n",
    "\n",
    "        # Clear memory\n",
    "        del loss, predictions, x_batch, y_batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Concatenate all predictions and observations\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    all_observations = torch.cat(all_observations)\n",
    "\n",
    "    # Compute additional metrics (e.g., NSE, RMSE)\n",
    "    nse = computeNSE(all_observations.numpy(), all_predictions.numpy())\n",
    "    rmse = torch.sqrt(torch.mean((1 - (all_predictions / all_observations)) ** 2)).item()\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Avg Loss: {avg_loss:.4f}, NSE: {nse:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # Save model state\n",
    "    model_state.append(copy.deepcopy(lstm_model.state_dict()))\n",
    "\n",
    "    # Clear memory\n",
    "    del all_predictions, all_observations\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46babca1-11fb-4e15-b8be-3b39c2e40b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom sampler function \n",
    "class SubsetSampler(Sampler):\n",
    "    def __init__(self, indices, generator=None):\n",
    "        super(Sampler, self).__init__()\n",
    "        self.indices = indices\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in self.indices:\n",
    "            yield i\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "# Create DataLoader for testing data\n",
    "test_indices = list(range(len(test_dataset)))  # Indices for all test data\n",
    "test_sampler = SubsetSampler(test_indices)  # Using the custom SubsetSampler\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=N, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e9d80f7-fb55-48eb-aa83-9bae4c9481ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Last Epoch: 100\n",
      "Testing results saved to: /N/lustre/project/proj-212/praval/Global_sediment_flux/lstm_model/codes/cluster/Hydrogeosciences_codes/tr_0.7_bs_32_id_85_hd_256_nl_1_lr_0.0001_sl_30_ep_100_se_1_last_epoch/testing/testing_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the model state from the last epoch\n",
    "last_epoch = len(model_state)  # Epochs start from 1\n",
    "print(f\"Loading Last Epoch: {last_epoch}\")\n",
    "\n",
    "last_state_dict = model_state[last_epoch - 1]  # Adjust for 0-based indexing\n",
    "lstm_model.load_state_dict(last_state_dict, strict=True)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# Define the folder structure for saving testing results\n",
    "base_path = \"/N/lustre/project/proj-212/praval/Global_sediment_flux/lstm_model/codes/cluster/Hydrogeosciences_codes/\"\n",
    "folder_name = f\"tr_{x}_bs_{N}_id_{input_dim}_hd_{hidden_dim}_nl_{n_layers}_lr_{learning_rate}_sl_{Lseq}_ep_{epochs}_se_{seed}_last_epoch\"\n",
    "testing_save_path = os.path.join(base_path, folder_name, \"testing\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(testing_save_path, exist_ok=True)\n",
    "\n",
    "# Collect predictions, observations, and COMIDs during testing\n",
    "testing_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (comid, x_batch, y_batch, y_date) in enumerate(test_dataloader):\n",
    "        # Move inputs to device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device).view(-1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        out = lstm_model(x_batch)\n",
    "\n",
    "        # Collect predictions, true observations, and COMID\n",
    "        for i in range(len(comid)):\n",
    "            max_y = comid_max_y_values.get(comid[i].item(), 1.0)  # Default to 1.0 if not found\n",
    "            observation = y_batch[i].item() * max_y\n",
    "            prediction = max(out[i].item() * max_y, 0.001)  # Clamp negative values to zero\n",
    "            date = datetime.date.fromordinal(int(y_date[i].item())).strftime('%m/%d/%Y')\n",
    "            testing_results.append({\n",
    "                \"COMID\": comid[i].item(),\n",
    "                \"Observation\": observation,\n",
    "                \"Prediction\": prediction,\n",
    "                \"Date\": date \n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "testing_results_df = pd.DataFrame(testing_results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "testing_results_file = os.path.join(testing_save_path, \"testing_results.csv\")\n",
    "testing_results_df.to_csv(testing_results_file, index=False)\n",
    "\n",
    "print(f\"Testing results saved to: {testing_results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c3edff7-792f-4488-a051-584e390d67ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results saved to: /N/lustre/project/proj-212/praval/Global_sediment_flux/lstm_model/codes/cluster/Hydrogeosciences_codes/tr_0.7_bs_32_id_85_hd_256_nl_1_lr_0.0001_sl_30_ep_100_se_1_last_epoch/training/training_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions and observations for the training dataset as well\n",
    "\n",
    "# Define the folder structure for saving training results\n",
    "training_save_path = os.path.join(base_path, folder_name, \"training\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(training_save_path, exist_ok=True)\n",
    "\n",
    "# Collect predictions, observations, and COMIDs during training evaluation\n",
    "training_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (comid, x_batch, y_batch, y_date) in enumerate(train_dataloader):\n",
    "        # Move inputs to device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device).view(-1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        out = lstm_model(x_batch)\n",
    "\n",
    "        # Collect predictions, true observations, and COMID\n",
    "        for i in range(len(comid)):\n",
    "            max_y = comid_max_y_values.get(comid[i].item(), 1.0)  # Default to 1.0 if not found\n",
    "            observation = y_batch[i].item() * max_y\n",
    "            prediction = max(out[i].item() * max_y, 0.001)  # Clamp negative values to zero\n",
    "            date = datetime.date.fromordinal(int(y_date[i].item())).strftime('%m/%d/%Y')\n",
    "            training_results.append({\n",
    "                \"COMID\": comid[i].item(),\n",
    "                \"Observation\": observation,\n",
    "                \"Prediction\": prediction,\n",
    "                \"Date\": date \n",
    "            })\n",
    "\n",
    "# Convert training results to a DataFrame\n",
    "training_results_df = pd.DataFrame(training_results)\n",
    "\n",
    "# Save training results to a CSV file\n",
    "training_results_file = os.path.join(training_save_path, \"training_results.csv\")\n",
    "training_results_df.to_csv(training_results_file, index=False)\n",
    "\n",
    "print(f\"Training results saved to: {training_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5d9f6-6102-449c-a1b0-44b56f409acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
